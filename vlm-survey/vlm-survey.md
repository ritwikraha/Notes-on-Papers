# Vision Language Models

<img width="698" alt="Screenshot 2024-06-17 at 8 25 27 PM" src="https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/eb344545-aceb-4f56-880f-235bf87c149e">


### 1. Introduction
**Description:** The introduction outlines the progression of large language models (LLMs) and their extension into the visual domain, discussing the challenges of integrating spatial understanding and attributes into these models. It emphasizes the ongoing research into making VLMs reliable.

### 2. The Families of VLMs

<img width="458" alt="Screenshot 2024-06-18 at 12 08 03 AM" src="https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/7afc66c6-ca5f-4c5a-8e8c-c463bb93d679">

**Subtopics:**
- Contrastive-based VLMs
Description: These models learn a joint embedding space for images and text by contrasting positive (matching image-text pairs) and negative (non-matching pairs) examples.
Models: CLIP, SigLIP, Llip, SLIP, CLIP-rocket
Architectures: Typically use separate image and text encoders (e.g., Vision Transformer for images, Transformer for text) followed by a projection layer to map to the joint embedding space.
Datasets: Large-scale web-scraped image-text datasets (e.g., LAION, Conceptual Captions, ALIGN)
Repositories: OpenCLIP (https://github.com/mlfoundations/open_clip)

- VLMs with Masking Objectives
Description: These models are trained to reconstruct masked parts of the input (either image patches or text tokens) given the unmasked parts.
Models: FLAVA, MaskVLM
Architectures: Often use Transformer-based encoders and decoders for both image and text.
Datasets: Similar to contrastive-based VLMs, but may also include datasets with bounding box annotations (e.g., COCO, Visual Genome).
Generative-based VLMs
Description: These models are trained to generate either text (captions) or images (or both) given the other modality.
Models: CoCa (text generation), Chameleon, CM3leon (multimodal generation), Stable Diffusion, Imagen, Parti (image generation)
Architectures: Autoregressive models (e.g., Transformer-based decoders) or diffusion models.
Datasets: Similar to contrastive-based VLMs, but may also include datasets with interleaved text and images (e.g., OBELICS, MMC4).

- VLMs from Pretrained Backbones
Description: These models leverage pretrained language models (LLMs) and/or image encoders, and learn a mapping between their representations.
Models: Frozen, MiniGPT-4, MiniGPT-5, MiniGPT-v2, Qwen-VL, BLIP-2, LLaVA, InstructBLIP, OpenFlamingo, Otter
Architectures: Typically use a pretrained LLM (e.g., Llama, OPT, Vicuna) and a pretrained image encoder (e.g., CLIP ViT, Q-Former), with a mapping network (e.g., linear projection, cross-attention) in between.
Datasets: May use smaller, curated datasets for fine-tuning (e.g., Conceptual Captions, SBU, LAION, MIMIC-IT).

### 3. A Guide to VLM Training


<img width="422" alt="Screenshot 2024-06-18 at 12 09 29 AM" src="https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/a5d0c6e7-9e05-4de0-a5de-1ff7d5178fe8">


   **Data:** Using synthetic data generated by image captioning models (e.g., BLIP, LLaVA) or text-to-image models (e.g., Stable Diffusion) to improve caption quality or augment the dataset.
    * **Augmentation:** Applying data augmentation techniques (e.g., SLIP, CLIP-rocket) to improve robustness and generalization.
    * **Interleaved Data:** Using datasets with interleaved text and images (e.g., OBELICS, MMC4) for training models like Flamingo and MM1.
    * **Human Annotation:** Leveraging human expertise for fine-grained annotations (e.g., DCI dataset).
* **Software:**
    * **Repositories:** OpenCLIP, transformers
    * **Compute Resources:** Typically require significant compute resources (e.g., 64+ GPUs).
    * **Speeding Up Training:** Using techniques like torch.compile, xformers library, efficient data loading (e.g., FFCV), and masking.
* **Model Selection:**
    * **Contrastive Models:** Good for image-text retrieval and as a base for further development. Require large datasets and batch sizes.
    * **Masking Models:** Can learn joint distributions without batch dependency, but may be less efficient.
    * **Generative Models:** Can generate images or captions, providing insights into learned representations. Computationally expensive to train.
    * **Pretrained Backbones:** Efficient when resources are limited, but may inherit biases or hallucinations from pretrained models.
* **Improving Grounding:**
    * **Bounding Boxes:** Using datasets with bounding box annotations (e.g., X-VLM) to improve object localization.
    * **Negative Captioning:** Using negative captions (e.g., ARO benchmark) to improve differentiation between correct and incorrect pairings.
* **Improving Alignment:**
    * **Instruction Tuning:** Fine-tuning on datasets with instructions and desired responses (e.g., LLaVA, InstructBLIP).
    * **RLHF:** Using a reward model to align model outputs with human preferences (e.g., LLaVA-RLHF).
    * **Multimodal In-Context Learning:** Providing examples as context for few-shot learning (e.g., Otter model).
* **Improving Text-Rich Image Understanding:**
    * **Fine-Grained Text-Rich Data:** Using datasets with text-rich images (e.g., LLaVAR) for instruction tuning.
    * **High-Resolution Images:** Developing models that can handle high-resolution images with fine-grained text (e.g., Monkey, Lumos).
* **Parameter-Efficient Fine-Tuning:**
    * **LoRA-based Methods:** Fine-tuning a small subset of parameters (e.g., LoRA, QLoRA, VeRA, DoRA).
    * **Prompt-based Methods:** Optimizing prompt context (e.g., CoOp, VPT).
    * **Adapter-based Methods:** Adding new modules between layers (e.g., CLIP-Adapter, VL-adapter, LLaMA-Adapter V2).
    * **Mapping-based Methods:** Learning a mapping between pretrained unimodal modules (e.g., LiMBeR, MAPL).

### 4. Approaches for Responsible VLM Evaluation

<img width="410" alt="Screenshot 2024-06-18 at 12 10 22 AM" src="https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/7c77e848-3ce5-49d1-a0e4-b3159396fe91">

* **Visio-Linguistic Abilities:**
    * **Image Captioning:** Evaluating caption quality using metrics like BLEU, ROUGE, or CLIPScore.
    * **Text-to-Image Consistency:** Assessing the alignment between generated images and text prompts using metrics like CLIPScore, TIFA, DSG, VPEval, or VQAScore.
    * **Visual Question Answering:** Answering natural language questions about images (e.g., VQAv2, TextVQA, GQA, OK-VQA, ScienceQA).
    * **Text-Centric VQA:** Answering questions about textual content in images (e.g., STVQA, TextVQA, OCRVQA, DocVQA, InfoVQA).
    * **Zero-Shot Image Classification:** Evaluating classification performance on unseen classes (e.g., ImageNet, CIFAR, Caltech).
    * **Visio-Linguistic Compositional Reasoning:** Assessing the ability to understand complex relationships between visual and textual elements (e.g., Winoground, ARO, SUGARCREPE).
    * **Dense Captioning and Crop-Caption Matching:** Evaluating fine-grained understanding of scene details (e.g., DCI dataset).
    * **Synthetic Data:** Using synthetic data (e.g., PUG) to control scene variations and evaluate robustness.
* **Bias and Disparities:**
    * **Classification Analysis:** Measuring biases in classification tasks related to people-related attributes (e.g., gender, skin tone, ethnicity).
    * **Embedding Analysis:** Analyzing relationships between text and image embeddings to uncover implicit biases (e.g., Grounded-WEAT, Grounded-SEAT).
    * **Language Biases:** Addressing unimodal shortcuts in multimodal benchmarks (e.g., VQA).
* **Hallucinations:**
    * **Object Hallucination:** Evaluating the presence of objects in captions that are not present in the image (e.g., CHAIR, POPE, CCEval).
    * **Model-Based Evaluation:** Using LLMs like GPT-4 to assess hallucinations (e.g., GAVIE, MMHal-Bench).
    * **Human Evaluation:** Leveraging human annotations to identify hallucinations.
* **Memorization:**
    * **Déjà Vu Memorization:** Measuring the ability to "remember" objects from training images even when not described in the caption (e.g., k-NN test).
    * **Regularization Techniques:** Evaluating the effectiveness of techniques like text randomization in mitigating memorization.
* **Red Teaming:**
    * **Adversarial Datasets:** Creating datasets aimed at eliciting harmful or undesirable outputs.
    * **Risk Taxonomies:** Organizing potential risks and developing leaderboards to benchmark model safety.
    * **Mitigation Strategies:** Developing post-processing or fine-tuning methods to address identified risks.

### 5. Extending VLMs to Videos

* **Challenges:**
    * **Computational Cost:** Processing videos is significantly more expensive than images.
    * **Data Scarcity:** Lack of large-scale, high-quality video-text datasets with temporal annotations.
    * **Temporal Understanding:** Current models often struggle to capture temporal relationships and dynamics.
* **Models and Architectures:**
    * **Early Fusion:** VideoBERT, MERLOT, VideoOFA
    * **Pretrained LLMs:** Video-LLaMA, MiniGPT4-Video, Video-LLaVA
* **Evaluation:**
    * **Video Captioning:** Similar to image captioning, but evaluating the ability to describe actions and events.
    * **Video Question Answering:** Answering questions about video content, including temporal localization of events.
    * **Reasoning and World Understanding:** Using synthetic data to evaluate reasoning about physical laws and spatio-temporal continuity (e.g., Grasp benchmark).
* **Opportunities:**
    * **Developing More Efficient Training Protocols:** Leveraging techniques like masking to reduce computational cost.
    * **Creating Richer Video-Text Datasets:** Generating captions that capture both scene content and temporal aspects.
    * **Developing More Challenging Benchmarks:** Evaluating reasoning capabilities and understanding of the world beyond simple scene description.

### 6. Conclusion
**Description:** Summarizes the key points discussed in the paper and outlines the future direction for VLM research.
This detailed breakdown of the paper's topics, subtopics, and related information should provide a comprehensive overview of the current state of VLM research and highlight key areas for future development. 


